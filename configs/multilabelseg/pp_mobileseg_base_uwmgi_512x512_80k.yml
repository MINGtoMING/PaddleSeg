batch_size: 32  # total batch size is 32
iters: 80000

train_dataset:
  type: COCO
  image_root: data/UWMGI/images/
  json_file: data/UWMGI/annotations/train.json
  add_background: False
  transforms:
    - type: ResizeStepScaling
      min_scale_factor: 0.5
      max_scale_factor: 2.0
      scale_step_size: 0.25
    - type: RandomPaddingCrop
      crop_size: [512, 512]
    - type: RandomHorizontalFlip
    - type: RandomDistort
      brightness_range: 0.4
      contrast_range: 0.4
      saturation_range: 0.4
    - type: Normalize
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
  mode: train

val_dataset:
  type: COCO
  image_root: data/UWMGI/images/
  json_file: data/UWMGI/annotations/val.json
  add_background: False
  transforms:
    - type: Resize
      target_size: [2048, 512]
      keep_ratio: True
      size_divisor: 32
    - type: Normalize
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
  mode: val

optimizer:
  type: AdamW
  weight_decay: 0.01
  custom_cfg:
  - name: pos_embed
    weight_decay_mult: 0.0
  - name: head
    lr_multi: 10.0
  - name: bn
    weight_decay_mult: 0.0

lr_scheduler:
  type: PolynomialDecay
  learning_rate: 0.0006
  end_lr: 0
  power: 1.0
  warmup_iters: 1500
  warmup_start_lr: 1.0e-6

loss:
  types:
    - type: MixedLoss
      losses:
        - type: BCELoss
        - type: LovaszHingeLoss
      coef: [0.5, 0.5]
  coef: [1]


model:
  type: PPMobileSeg
  num_classes: 3
  backbone:
    type: MobileSeg_Base
    inj_type: AAMSx8
    out_feat_chs: [64, 128, 192] 
    pretrained: https://bj.bcebos.com/paddleseg/dygraph/ade20k/pp_mobileseg/pretrain/model.pdparams
  upsample: intepolate # During exportation, you need to change it to vim for using VIM
